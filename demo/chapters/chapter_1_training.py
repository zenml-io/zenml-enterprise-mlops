"""Chapter 1: Train Locally.

Demonstrates:
- Clean developer experience (pure Python)
- Platform governance via hooks
- Automatic MLflow logging
- Model versioning in Model Control Plane
- Fast iteration on dev-stack (local orchestrator)

This is what a data scientist does day-to-day: iterate locally with fast
feedback loops before pushing code for CI/CD.
"""

import subprocess
import sys


def print_section(title: str):
    """Print section header."""
    print(f"\n{'‚îÄ' * 60}")
    print(f"  {title}")
    print(f"{'‚îÄ' * 60}\n")


def run():
    """Run Chapter 1: Local Training."""

    print_section("üéØ What We're Demonstrating")
    print("  üîß Workspace: enterprise-dev-staging")
    print("  üì¶ Stack: dev-stack (local orchestrator, GCS artifacts)")
    print(
        """
A data scientist iterates locally with fast feedback loops.

Key points to highlight:
  ‚úì Data scientists write PURE PYTHON - no framework wrappers
  ‚úì Platform governance is AUTOMATIC via hooks
  ‚úì MLflow experiment tracking via stack component (zero code!)
  ‚úì Model is versioned in the Model Control Plane
  ‚úì Same code runs locally AND in CI/CD (just different configs)
"""
    )

    print_section("üìù The Training Pipeline Code")
    print(
        """
Here's what the training pipeline looks like (src/pipelines/training.py):

    from governance.hooks import pipeline_success_hook, pipeline_failure_hook

    @pipeline(
        model=Model(name="breast_cancer_classifier"),
        on_success=pipeline_success_hook,    # ‚Üê Automatic governance
        on_failure=pipeline_failure_hook,    # ‚Üê Automatic alerting
    )
    def training_pipeline():
        X_train, X_test, y_train, y_test = load_data()
        X_train = validate_data_quality(X_train)  # ‚Üê Platform validation
        model = train_model(X_train, y_train)
        metrics = evaluate_model(model, X_test, y_test)
        validate_model_performance(metrics)       # ‚Üê Platform validation
        return model, metrics

Notice:
  ‚Ä¢ Clean, readable Python - no wrapper code
  ‚Ä¢ Governance is injected by platform team via hooks
  ‚Ä¢ Same pipeline runs on any stack (local, staging, production)
"""
    )

    print_section("üöÄ Running the Training Pipeline (local)")

    # Note: run.py automatically sets dev-stack for local environment
    print("  Stack will be set automatically by run.py based on environment")
    print("  ‚úÖ Environment: local ‚Üí Stack: dev-stack\n")

    print("Command: python run.py --pipeline training --environment local\n")

    try:
        result = subprocess.run(
            [sys.executable, "run.py", "--pipeline", "training", "--environment", "local"],
            capture_output=False,
            text=True,
            timeout=300,
        )

        if result.returncode == 0:
            print("\n‚úÖ Local training completed successfully!")
        else:
            print(f"\n‚ö†Ô∏è  Training finished with return code: {result.returncode}")

    except subprocess.TimeoutExpired:
        print("\n‚è±Ô∏è  Training timed out")
    except FileNotFoundError:
        print("\n‚ö†Ô∏è  run.py not found - running from wrong directory?")

    print_section("üìä What Happened Behind the Scenes")
    print(
        """
1. Pipeline executed with automatic governance:
   ‚Ä¢ Data quality validation checked for missing values, min rows
   ‚Ä¢ Model performance validation checked accuracy, precision, recall

2. MLflow logged everything automatically (via hook):
   ‚Ä¢ Model parameters (n_estimators, max_depth)
   ‚Ä¢ Metrics (accuracy, precision, recall, f1)
   ‚Ä¢ Model artifact

3. Model Control Plane recorded:
   ‚Ä¢ New model version created
   ‚Ä¢ Full lineage (data ‚Üí model ‚Üí metrics)
   ‚Ä¢ All metadata for audit trail

This is the fast inner loop. Now let's simulate pushing this
code as a PR and running it on the staging stack ‚Üí
"""
    )


if __name__ == "__main__":
    run()
