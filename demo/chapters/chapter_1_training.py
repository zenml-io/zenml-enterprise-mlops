"""Chapter 1: Train a Model.

Demonstrates:
- Clean developer experience (pure Python)
- Platform governance via hooks
- Automatic MLflow logging
- Model versioning in Model Control Plane
"""

import subprocess
import sys


def print_section(title: str):
    """Print section header."""
    print(f"\n{'‚îÄ' * 60}")
    print(f"  {title}")
    print(f"{'‚îÄ' * 60}\n")


def run():
    """Run Chapter 1: Training a Model."""

    print_section("üéØ What We're Demonstrating")
    print(
        """
In this chapter, we train a risk prediction model using the breast cancer dataset.

Key points to highlight:
  ‚úì Data scientists write PURE PYTHON - no framework wrappers
  ‚úì Platform governance is AUTOMATIC via hooks
  ‚úì MLflow experiment tracking via stack component (zero code!)
  ‚úì Slack notifications on success/failure
  ‚úì Model is versioned in the Model Control Plane
"""
    )

    print_section("üìù The Training Pipeline Code")
    print(
        """
Here's what the training pipeline looks like (src/pipelines/training.py):

    from governance.hooks import pipeline_success_hook, pipeline_failure_hook

    @pipeline(
        model=Model(name="patient_readmission_predictor"),
        on_success=pipeline_success_hook,    # ‚Üê Slack notification
        on_failure=pipeline_failure_hook,    # ‚Üê Slack alert + compliance log
    )
    def training_pipeline():
        X_train, X_test, y_train, y_test = load_data()
        X_train = validate_data_quality(X_train)  # ‚Üê Platform validation
        model = train_model(X_train, y_train)
        metrics = evaluate_model(model, X_test, y_test)
        validate_model_performance(metrics)       # ‚Üê Platform validation
        return model, metrics

Notice:
  ‚Ä¢ MLflow tracking is automatic (experiment_tracker in stack)
  ‚Ä¢ Slack alerts on success/failure (alerter in stack)
  ‚Ä¢ Just clean, readable Python
"""
    )

    print_section("üöÄ Running the Training Pipeline")
    print("Executing: python run.py --pipeline training\n")

    try:
        result = subprocess.run(
            [sys.executable, "run.py", "--pipeline", "training"],
            capture_output=False,
            text=True,
            timeout=300,
        )

        if result.returncode == 0:
            print("\n‚úÖ Training completed successfully!")
        else:
            print(f"\n‚ö†Ô∏è  Training finished with return code: {result.returncode}")

    except subprocess.TimeoutExpired:
        print("\n‚è±Ô∏è  Training timed out (this is normal for long runs)")
    except FileNotFoundError:
        print("\n‚ö†Ô∏è  run.py not found - running from wrong directory?")
        print("   Run from project root: cd zenml-enterprise-mlops")

    print_section("üìä What Happened Behind the Scenes")
    print(
        """
1. Pipeline executed with automatic governance:
   ‚Ä¢ Data quality validation checked for missing values, min rows
   ‚Ä¢ Model performance validation checked accuracy, precision, recall

2. MLflow logged everything automatically (via hook):
   ‚Ä¢ Model parameters (n_estimators, max_depth)
   ‚Ä¢ Metrics (accuracy, precision, recall, f1)
   ‚Ä¢ Model artifact

3. Model Control Plane recorded:
   ‚Ä¢ New model version created
   ‚Ä¢ Full lineage (data ‚Üí model ‚Üí metrics)
   ‚Ä¢ Git commit (if code repo configured)
   ‚Ä¢ All metadata for audit trail

Next: Let's explore this in the Model Control Plane ‚Üí
"""
    )


if __name__ == "__main__":
    run()
