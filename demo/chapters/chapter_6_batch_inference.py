"""Chapter 6: Run Batch Inference.

Demonstrates:
- Loading production model by stage
- Automatic model versioning
- Prediction lineage
- Scheduled inference pattern

In two-workspace mode, this runs in enterprise-production workspace.
The workspace switch happens in run_demo.py before calling this chapter.
"""

import subprocess
import sys


def print_section(title: str):
    """Print section header."""
    print(f"\n{'â”€' * 60}")
    print(f"  {title}")
    print(f"{'â”€' * 60}\n")


def run(two_workspace: bool = False):
    """Run Chapter 6: Batch Inference."""

    print_section("ğŸ¯ What We're Demonstrating")

    if two_workspace:
        print("  ğŸ­ Workspace: enterprise-production\n")
        print(
            """
Batch inference runs in the PRODUCTION workspace using the imported model.

Key points to highlight:
  âœ“ Model was imported from dev-staging via cross-workspace promotion
  âœ“ Model loaded by STAGE, not version number
  âœ“ Inference lineage is preserved in the production workspace
  âœ“ Training lineage is preserved in dev-staging workspace
  âœ“ Scheduled via cron in GitOps workflow
"""
        )
    else:
        print(
            """
Batch inference uses the PRODUCTION model automatically.

Key points to highlight:
  âœ“ Model loaded by STAGE, not version number
  âœ“ Predictions linked to model version for lineage
  âœ“ Same code works as model versions change
  âœ“ Scheduled via cron in GitOps workflow
"""
        )

    print_section("ğŸ“ Batch Inference Pattern")
    print(
        """
The key pattern - load model by STAGE:

    @pipeline(
        model=Model(
            name="breast_cancer_classifier",
            version=ModelStages.PRODUCTION,  # â† Always uses current production
        ),
    )
    def batch_inference_pipeline():
        data = load_inference_data()
        predictions = run_predictions(data)
        save_predictions(predictions)

Benefits:
  â€¢ No code changes when model is updated
  â€¢ Predictions automatically linked to model version
  â€¢ Easy to switch to staging for testing
  â€¢ Complete lineage maintained
"""
    )

    if two_workspace:
        print_section("ğŸ”— Lineage Across Workspaces")
        print(
            """
  enterprise-dev-staging              enterprise-production
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Training Lineage:      â”‚         â”‚ Inference Lineage:           â”‚
  â”‚   data â†’ features â†’   â”‚         â”‚   model â†’ predictions â†’     â”‚
  â”‚   model â†’ metrics     â”‚  audit  â”‚   saved results             â”‚
  â”‚                        â”‚  trail  â”‚                              â”‚
  â”‚ Full training history  â”‚ â—€â”€â”€â”€â”€â”€â–¶ â”‚ Model metadata links back   â”‚
  â”‚ preserved here         â”‚         â”‚ to source workspace         â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  Training lineage: Fully preserved in dev-staging
  Inference lineage: Fully preserved in production
  Audit trail: Production model metadata links back to source
"""
        )

    print_section("ğŸ” Current Production Model")

    try:
        from zenml.client import Client
        from zenml.enums import ModelStages

        client = Client()

        try:
            prod = client.get_model_version(
                "breast_cancer_classifier",
                ModelStages.PRODUCTION,
            )
            print(f"  Production Model: v{prod.number}")
            print(f"  Created: {str(prod.created)[:19]}")

            # Show metrics
            metrics = prod.run_metadata
            print("\n  Performance:")
            for key in ["accuracy", "precision", "recall"]:
                if key in metrics:
                    val = metrics[key]
                    val = float(val.value if hasattr(val, "value") else val)
                    print(f"    {key}: {val:.4f}")

            # Show source metadata if cross-workspace
            if two_workspace:
                print("\n  Source Metadata:")
                for key in [
                    "source_workspace",
                    "source_version",
                    "source_stage",
                    "promotion_timestamp",
                ]:
                    if key in metrics:
                        val = metrics[key]
                        val = val.value if hasattr(val, "value") else val
                        print(f"    {key}: {val}")

        except KeyError:
            print("  âš ï¸  No production model found. Run Chapter 5 first!")
            return

    except Exception as e:
        print(f"Could not check model: {e}")
        return

    print_section("ğŸš€ Running Batch Inference")

    # Ensure we're on default stack (or gcp-stack in production workspace)
    print("  Setting stack to 'default'...")
    subprocess.run(["zenml", "stack", "set", "default"], capture_output=True)
    print("  âœ… Stack: default\n")

    print("Executing: python run.py --pipeline batch_inference\n")

    try:
        result = subprocess.run(
            [sys.executable, "run.py", "--pipeline", "batch_inference"],
            capture_output=False,
            text=True,
            timeout=300,
        )

        if result.returncode == 0:
            print("\nâœ… Batch inference completed!")
        else:
            print(f"\nâš ï¸  Inference finished with code: {result.returncode}")

    except subprocess.TimeoutExpired:
        print("\nâ±ï¸  Inference timed out")
    except FileNotFoundError:
        print("\nâš ï¸  run.py not found")

    print_section("ğŸ“… Scheduled Inference Pattern")

    if two_workspace:
        print(
            """
In production, batch inference runs on a schedule in the production workspace:

  .github/workflows/batch-inference.yml:

    env:
      ZENML_STORE_URL: ${{ secrets.ZENML_PRODUCTION_URL }}
      ZENML_STORE_API_KEY: ${{ secrets.ZENML_PRODUCTION_API_KEY }}

    on:
      schedule:
        - cron: '0 6 * * *'  # Daily at 6 AM UTC

    jobs:
      inference:
        steps:
          - run: python run.py --pipeline batch_inference

This enables:
  â€¢ Daily risk scoring in the production workspace
  â€¢ Automatic use of latest production model
  â€¢ Inference lineage preserved in production
  â€¢ Training lineage preserved in dev-staging
  â€¢ Complete audit trail across workspaces
"""
        )
    else:
        print(
            """
In production, batch inference runs on a schedule:

  .github/workflows/batch-inference.yml:

    on:
      schedule:
        - cron: '0 6 * * *'  # Daily at 6 AM UTC

    jobs:
      inference:
        steps:
          - run: python run.py --pipeline batch_inference

This enables:
  â€¢ Daily risk scoring for all patients
  â€¢ Automatic use of latest production model
  â€¢ Complete lineage for every prediction
  â€¢ Easy audit of what model made what predictions
"""
        )

    print_section("ğŸ‰ Demo Complete!")
    print(
        """
You've seen the complete enterprise MLOps workflow:

  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚                                                             â”‚
  â”‚  1. TRAIN          Clean Python, automatic governance       â”‚
  â”‚       â†“            (enterprise-dev-staging)                 â”‚
  â”‚  2. VERSION        Model Control Plane tracks everything    â”‚
  â”‚       â†“            (enterprise-dev-staging)                 â”‚
  â”‚  3. STAGING        Validation gates, PR-triggered           â”‚
  â”‚       â†“            (enterprise-dev-staging)                 â”‚
  â”‚  4. COMPARE        Champion/Challenger for safe rollouts    â”‚
  â”‚       â†“            (enterprise-dev-staging)                 â”‚
  â”‚  5. PRODUCTION     Cross-workspace export/import            â”‚
  â”‚       â†“            (dev-staging â†’ production)               â”‚
  â”‚  6. INFERENCE      Scheduled batch, complete lineage        â”‚
  â”‚                    (enterprise-production)                   â”‚
  â”‚                                                             â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Key Enterprise Benefits:
  âœ“ Clean developer experience (no wrapper code)
  âœ“ Automatic governance (hooks enforce standards)
  âœ“ Complete audit trail (compliance-ready)
  âœ“ GitOps workflows (git = source of truth)
  âœ“ Safe rollouts (champion/challenger pattern)
  âœ“ ZenML version upgrade isolation (2 workspaces)

Dashboard: zenml login
Docs: docs/ARCHITECTURE.md
"""
    )


if __name__ == "__main__":
    run()
