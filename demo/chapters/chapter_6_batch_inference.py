"""Chapter 6: Run Batch Inference.

Demonstrates:
- Loading production model by stage
- Automatic model versioning
- Prediction lineage
- Scheduled inference pattern
"""

import subprocess
import sys


def print_section(title: str):
    """Print section header."""
    print(f"\n{'â”€' * 60}")
    print(f"  {title}")
    print(f"{'â”€' * 60}\n")


def run():
    """Run Chapter 6: Batch Inference."""

    print_section("ğŸ¯ What We're Demonstrating")
    print(
        """
Batch inference uses the PRODUCTION model automatically.

Key points to highlight:
  âœ“ Model loaded by STAGE, not version number
  âœ“ Predictions linked to model version for lineage
  âœ“ Same code works as model versions change
  âœ“ Scheduled via cron in GitOps workflow
"""
    )

    print_section("ğŸ“ Batch Inference Pattern")
    print(
        """
The key pattern - load model by STAGE:

    @pipeline(
        model=Model(
            name="breast_cancer_classifier",
            version=ModelStages.PRODUCTION,  # â† Always uses current production
        ),
    )
    def batch_inference_pipeline():
        data = load_inference_data()
        predictions = run_predictions(data)
        save_predictions(predictions)

Benefits:
  â€¢ No code changes when model is updated
  â€¢ Predictions automatically linked to model version
  â€¢ Easy to switch to staging for testing
  â€¢ Complete lineage maintained
"""
    )

    print_section("ğŸ” Current Production Model")

    try:
        from zenml.client import Client
        from zenml.enums import ModelStages

        client = Client()

        try:
            prod = client.get_model_version(
                "breast_cancer_classifier",
                ModelStages.PRODUCTION,
            )
            print(f"  Production Model: v{prod.number}")
            print(f"  Created: {str(prod.created)[:19]}")

            # Show metrics
            metrics = prod.run_metadata
            print("\n  Performance:")
            for key in ["accuracy", "precision", "recall"]:
                if key in metrics:
                    val = metrics[key]
                    val = float(val.value if hasattr(val, "value") else val)
                    print(f"    {key}: {val:.4f}")

        except KeyError:
            print("  âš ï¸  No production model found. Run Chapter 5 first!")
            return

    except Exception as e:
        print(f"Could not check model: {e}")
        return

    print_section("ğŸš€ Running Batch Inference")
    print("Executing: python run.py --pipeline batch_inference\n")

    try:
        result = subprocess.run(
            [sys.executable, "run.py", "--pipeline", "batch_inference"],
            capture_output=False,
            text=True,
            timeout=300,
        )

        if result.returncode == 0:
            print("\nâœ… Batch inference completed!")
        else:
            print(f"\nâš ï¸  Inference finished with code: {result.returncode}")

    except subprocess.TimeoutExpired:
        print("\nâ±ï¸  Inference timed out")
    except FileNotFoundError:
        print("\nâš ï¸  run.py not found")

    print_section("ğŸ“… Scheduled Inference Pattern")
    print(
        """
In production, batch inference runs on a schedule:

  .github/workflows/batch-inference.yml:

    on:
      schedule:
        - cron: '0 6 * * *'  # Daily at 6 AM UTC

    jobs:
      inference:
        steps:
          - run: python run.py --pipeline batch_inference

This enables:
  â€¢ Daily risk scoring for all patients
  â€¢ Automatic use of latest production model
  â€¢ Complete lineage for every prediction
  â€¢ Easy audit of what model made what predictions
"""
    )

    print_section("ğŸ‰ Demo Complete!")
    print(
        """
You've seen the complete enterprise MLOps workflow:

  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚                                                             â”‚
  â”‚  1. TRAIN          Clean Python, automatic governance       â”‚
  â”‚       â†“                                                     â”‚
  â”‚  2. VERSION        Model Control Plane tracks everything    â”‚
  â”‚       â†“                                                     â”‚
  â”‚  3. STAGING        Validation gates, PR-triggered           â”‚
  â”‚       â†“                                                     â”‚
  â”‚  4. COMPARE        Champion/Challenger for safe rollouts    â”‚
  â”‚       â†“                                                     â”‚
  â”‚  5. PRODUCTION     Release-triggered, higher bar            â”‚
  â”‚       â†“                                                     â”‚
  â”‚  6. INFERENCE      Scheduled batch, complete lineage        â”‚
  â”‚                                                             â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Key Enterprise Benefits:
  âœ“ Clean developer experience (no wrapper code)
  âœ“ Automatic governance (hooks enforce standards)
  âœ“ Complete audit trail (compliance-ready)
  âœ“ GitOps workflows (git = source of truth)
  âœ“ Safe rollouts (champion/challenger pattern)

Dashboard: zenml login
Docs: docs/ARCHITECTURE.md
"""
    )


if __name__ == "__main__":
    run()
