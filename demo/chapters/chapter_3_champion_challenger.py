"""Chapter 3: Champion vs Challenger.

Demonstrates:
- Safe model rollouts via A/B comparison
- Comparing new staging-trained model vs current staging model
- Data-driven promotion decisions
- Pipeline snapshots for immutable deployments (Pro)

This mirrors the validation step after train-staging.yml completes:
  - test-batch-inference.yml validates the new model
  - Champion/Challenger compares before promotion
"""

import subprocess
import sys


def print_section(title: str):
    """Print section header."""
    print(f"\n{'â”€' * 60}")
    print(f"  {title}")
    print(f"{'â”€' * 60}\n")


def run():
    """Run Chapter 3: Champion/Challenger comparison."""

    print_section("ğŸ¯ What We're Demonstrating")
    print("  ğŸ”§ Workspace: enterprise-dev-staging")
    print("  ğŸ“¦ Stack: dev-stack (local orchestrator, GCS artifacts)")
    print(
        """
After the staging training (Ch2), we validate the new model before promoting.

In the GitOps flow:
  1. train-staging.yml trains the model on staging-stack
  2. test-batch-inference.yml validates inference works
  3. Champion/Challenger compares new vs current staging model
  4. If safe â†’ merge PR â†’ promote to staging (Ch4)

We compare the newly trained model (challenger) against the current
staging model (champion) to ensure we're not regressing.
"""
    )

    print_section("ğŸ¥Š Champion vs Challenger Pattern")
    print(
        """
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚                      INFERENCE DATA                         â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â–¼                           â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚     CHAMPION      â”‚       â”‚    CHALLENGER     â”‚
  â”‚   (Current        â”‚       â”‚   (Newly trained  â”‚
  â”‚    Staging)       â”‚       â”‚    from Ch2)      â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚                           â”‚
              â–¼                           â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚   Predictions A   â”‚       â”‚   Predictions B   â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚                           â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚      COMPARISON           â”‚
              â”‚  â€¢ Agreement Rate         â”‚
              â”‚  â€¢ Probability Diff       â”‚
              â”‚  â€¢ Risk Assessment        â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚   PROMOTION DECISION      â”‚
              â”‚  SAFE / REVIEW / CAUTION  â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
"""
    )

    print_section("ğŸ“Š Current Model Versions (dev-staging workspace)")

    try:
        from zenml.client import Client
        from zenml.enums import ModelStages

        client = Client()

        # Check staging (champion)
        staging = None
        try:
            staging = client.get_model_version(
                "breast_cancer_classifier",
                ModelStages.STAGING,
            )
            print(f"  ğŸ† Champion (Current Staging):  v{staging.number}")
        except KeyError:
            print("  ğŸ† Champion (Current Staging):  None")

        # Check latest (challenger = newly trained from ch2)
        latest = None
        try:
            latest = client.get_model_version(
                "breast_cancer_classifier",
                ModelStages.LATEST,
            )
            print(f"  ğŸ¥Š Challenger (Latest trained):  v{latest.number}")
        except KeyError:
            print("  ğŸ¥Š Challenger (Latest trained):  None - run Chapter 1/2 first")

    except Exception as e:
        print(f"Could not check models: {e}")
        staging = None

    print_section("ğŸš€ Running Champion/Challenger Comparison")

    # Ensure we're on dev-stack
    print("  Setting stack to 'dev-stack'...")
    subprocess.run(["zenml", "stack", "set", "dev-stack"], capture_output=True)
    print("  âœ… Stack: dev-stack\n")

    # The champion_challenger_pipeline compares STAGING (champion) vs LATEST (challenger).
    # If no staging exists (first run), show narrative about what happens.
    if staging is not None:
        print("Command: python run.py --pipeline champion_challenger\n")
        try:
            result = subprocess.run(
                [sys.executable, "run.py", "--pipeline", "champion_challenger"],
                capture_output=False,
                text=True,
                timeout=300,
            )
            if result.returncode == 0:
                print("\nâœ… Champion/Challenger comparison completed!")
            else:
                print(f"\nâš ï¸  Comparison finished with code: {result.returncode}")
        except subprocess.TimeoutExpired:
            print("\nâ±ï¸  Comparison timed out")
        except FileNotFoundError:
            print("\nâš ï¸  run.py not found")
    else:
        print(
            """
Note: The champion_challenger_pipeline compares STAGING vs LATEST models.
No staging model exists yet (first demo run).

This is expected! On first run:
  - Ch1/Ch2 train a model (becomes LATEST)
  - Ch4 will promote it to STAGING
  - On subsequent runs, Ch3 will compare new LATEST vs current STAGING

Command: python run.py --pipeline champion_challenger

Expected comparison report (on subsequent runs):

  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  CHAMPION vs CHALLENGER COMPARISON REPORT              â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚  Champion (Current Staging):  v1                       â”‚
  â”‚  Challenger (Latest Trained): v2                       â”‚
  â”‚                                                        â”‚
  â”‚  Total Samples: 1,000                                  â”‚
  â”‚  Agreement Rate: 94.2%                                 â”‚
  â”‚  Avg Probability Diff: 0.032                           â”‚
  â”‚  Max Probability Diff: 0.156                           â”‚
  â”‚                                                        â”‚
  â”‚  RECOMMENDATION: SAFE TO PROMOTE                       â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
"""
        )

    print_section("ğŸ“¸ Pipeline Snapshots (Pro Feature)")
    print(
        """
In CI/CD, validations can be deployed as immutable snapshots:

  # Build snapshot for staging validation
  python scripts/build_snapshot.py \\
      --pipeline champion_challenger \\
      --environment staging \\
      --stack staging-stack

  # Build batch inference snapshot for validation
  python scripts/build_snapshot.py \\
      --pipeline batch_inference \\
      --environment staging \\
      --stack staging-stack \\
      --run

This mirrors test-batch-inference.yml which:
  1. Runs after train-staging.yml completes
  2. Validates batch inference with the new model
  3. Uses staging-stack (Vertex AI) for production-like testing
  4. Builds an immutable snapshot for reproducibility
"""
    )

    print_section("ğŸ“‹ Interpreting the Comparison")
    print(
        """
  RECOMMENDATION LEVELS
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  â€¢ SAFE TO PROMOTE: Agreement â‰¥95%, Prob diff <5%
    â†’ Merge PR, promote to staging

  â€¢ REVIEW RECOMMENDED: Agreement 85-95%
    â†’ Review disagreement cases, then decide

  â€¢ CAUTION: Agreement <85%
    â†’ Investigate before promoting

This data-driven approach reduces production risk!

Next: Let's promote the validated model to staging â†’
"""
    )


if __name__ == "__main__":
    run()
